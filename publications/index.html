<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="JaSyZ6iL1QHnI52LuQlz1BdkG_kEuhKKNcWq9xrsmhU"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Amir Hossein Kargaran</title> <meta name="author" content="Amir Hossein Kargaran"/> <meta name="description" content="publications by categories in reversed chronological order."/> <meta name="keywords" content="Amir Hossein Kargaran, Amirhossein Kargaran Khouzani, kargaranamir, Amir Kargaran, Amir-Hossein-Kargaran, Amir-Kargaran"/> <meta property="og:site_name" content="Amir Hossein Kargaran"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Amir Hossein Kargaran | publications"/> <meta property="og:url" content="https://kargaranamir.github.io/publications/"/> <meta property="og:description" content="publications by categories in reversed chronological order."/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="publications"/> <meta name="twitter:description" content="publications by categories in reversed chronological order."/> <meta name="twitter:site" content="@amir_nlp"/> <meta name="twitter:creator" content="@amir_nlp"/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Amir Hossein  Kargaran"
        },
        "url": "https://kargaranamir.github.io/publications/",
        "@type": "WebSite",
        "description": "publications by categories in reversed chronological order.",
        "headline": "publications",
        "sameAs": ["https://orcid.org/0000-0001-6253-1315", "https://scholar.google.com/citations?user=2idwpjcAAAAJ", "https://www.researchgate.net/profile/Amir_Hossein_Kargaran", "https://github.com/kargaranamir", "https://www.linkedin.com/in/amirkargaran", "https://twitter.com/amir_nlp", "https://huggingface.co/kargaranamir", "https://gitlab.com/kargaranamir", "https://dblp.uni-trier.de/pers/hd/k/Kargaran:Amir_Hossein", "http://arxiv.org/a/kargaran_a_1", "https://stackoverflow.com/users/16478538", "https://www.kaggle.com/kargaranamir", "https://myanimelist.net/animelist/kargaranamir", "https://www.last.fm/user/kargaranamir", "https://open.spotify.com/user/21wduv4jr5477ajojgxej4cha?si=L3iavpzJQleF_ih-tnIxmA", "https://letterboxd.com/kargaranamir", "https://www.pinterest.com/kargaranamir", "https://unsplash.com/@kargaranamir", "https://instagram.com/kargaranamir", "https://facebook.com/kargaranamir", "https://www.semanticscholar.org/author/98623604", "https://discord.com/users/818283252757823528"],
        "name": "Amir Hossein  Kargaran",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🕸</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://kargaranamir.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://kargaranamir.github.io/"><span class="font-weight-bold">Amir Hossein</span> Kargaran</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/social/">social</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">COLING 2025</abbr></div> <div id="liu-etal-2025-transliterations" class="col-sm-8"> <div class="title">How Transliterations Improve Crosslingual Alignment</div> <div class="author">Yihong Liu, Mingyang Wang,  <b>Amir Hossein Kargaran</b>, Ayyoob Imani, Orgest Xhelili, Haotian Ye, Chunlan Ma, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In Proceedings of the 31st International Conference on Computational Linguistics</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2025.coling-main.165/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACL</a> </div> <div class="abstract hidden"> <p>Recent studies have shown that post-aligning multilingual pretrained language models (mPLMs) using alignment objectives on both original and transliterated data can improve crosslingual alignment. This improvement further leads to better crosslingual transfer performance. However, it remains unclear how and why a better crosslingual alignment is achieved, as this technique only involves transliterations, and does not use any parallel data. This paper attempts to explicitly evaluate the crosslingual alignment and identify the key elements in transliteration-based approaches that contribute to better performance. For this, we train multiple models under varying setups for two pairs of related languages: (1) Polish and Ukrainian and (2) Hindi and Urdu. To assess alignment, we define four types of similarities based on sentence representations. Our experimental results show that adding transliterations alone improves the overall similarities, even for random sentence pairs. With the help of auxiliary transliteration-based alignment objectives, especially the contrastive objective, the model learns to distinguish matched from random pairs, leading to better crosslingual alignment. However, we also show that better alignment does not always yield better downstream performance, suggesting that further research is needed to clarify the connection between alignment and performance. The code implementation is based on https://github.com/cisnlp/Transliteration-PPA.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv 2025</abbr></div> <div id="liu-2025-relation" class="col-sm-8"> <div class="title">On Relation-Specific Neurons in Large Language Models</div> <div class="author">Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang,  <b>Amir Hossein Kargaran</b>, Sascha Rothe, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>arXiv preprint</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.17355" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/relation-specific-neurons" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself – independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation r on the LLM’s ability to handle (1) facts whose relation is r and (2) facts whose relation is a different relation r′≠r. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. (i) Neuron cumulativity. The neurons for r present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in r. (ii) Neuron versatility. Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. (iii) Neuron interference. Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv 2025</abbr></div> <div id="liu-2025-tracing" class="col-sm-8"> <div class="title">Tracing Multilingual Factual Knowledge Acquisition in Pretraining</div> <div class="author">Yihong Liu, Mingyang Wang,  <b>Amir Hossein Kargaran</b>, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>arXiv preprint</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.17355" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/multilingual-fact-tracing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) are capable of recalling multilingual factual knowledge present in their pretraining data. However, most studies evaluate only the final model, leaving the development of factual recall and crosslingual consistency throughout pretraining largely unexplored. In this work, we trace how factual recall and crosslingual consistency evolve during pretraining, focusing on OLMo-7B as a case study. We find that both accuracy and consistency improve over time for most languages. We show that this improvement is primarily driven by the fact frequency in the pretraining corpus: more frequent facts are more likely to be recalled correctly, regardless of language. Yet, some low-frequency facts in non-English languages can still be correctly recalled. Our analysis reveals that these instances largely benefit from crosslingual transfer of their English counterparts – an effect that emerges predominantly in the early stages of pretraining. We pinpoint two distinct pathways through which multilingual factual knowledge acquisition occurs: (1) frequency-driven learning, which is dominant and language-agnostic, and (2) crosslingual transfer, which is limited in scale and typically constrained to relation types involving named entities. We release our code and data to facilitate further research at https://github.com/cisnlp/multilingual-fact-tracing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">COLM 2025</abbr></div> <div id="penedo-2025-fineweb2" class="col-sm-8"> <div class="title">FineWeb2: One Pipeline to Scale Them All–Adapting Pre-Training Data Processing to Every Language</div> <div class="author">Guilherme Penedo, Hynek Kydlı́ček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan,  <b>Amir Hossein Kargaran</b>, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf </div> <div class="periodical"> <em>In Second Conference on Language Modeling</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.20920" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/huggingface/fineweb-2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2025</abbr></div> <div id="kargaran-2025-programming" class="col-sm-8"> <div class="title">How Programming Concepts and Neurons Are Shared in Code Language Models</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Yihong Liu, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.01074" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/code-specific-neurons" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model’s concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model’s concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2025</abbr></div> <div id="kargaran-2025-mexa" class="col-sm-8"> <div class="title">MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.05873" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/Mexa" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://huggingface.co/spaces/cis-lmu/Mexa" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.</p> </div> </div> </div> </li> </ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS 2024</abbr></div> <div id="kargaran-2024-glotcc" class="col-sm-8"> <div class="title">GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages</div> <div class="author"> <b>Amir Hossein Kargaran</b>, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em></em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.23825" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/GlotCC" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://huggingface.co/datasets/cis-lmu/GlotCC-v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it— including the pipeline, language identification model, and filters—available to the research community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1, Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2024</abbr></div> <div id="kargaran-2024-masklid" class="col-sm-8"> <div class="title">MaskLID: Code-Switching Language Identification through Iterative Masking</div> <div class="author"> <b>Amir Hossein Kargaran</b>, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.06263" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/paper-masklid-acl-2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/cisnlp/MaskLID" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://huggingface.co/spaces/cis-lmu/MaskLID" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>We present MaskLID, a simple, yet effective, code-switching (CS) language identification (LID) method. MaskLID does not require any training and is designed to complement current high-performance sentence-level LIDs. Sentence-level LIDs are classifiers trained on monolingual texts to provide single labels, typically using a softmax layer to turn scores into probabilities. However, in cases where a sentence is composed in both L1 and L2 languages, the LID classifier often only returns the dominant label L1. To address this limitation, MaskLID employs a strategy to mask text features associated with L1, allowing the LID to classify the text as L2 in the next round. This method uses the LID itself to identify the features that require masking and does not rely on any external resource. In this work, we explore the use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that are both based on the FastText architecture. Code and demo are available at https://github.com/cisnlp/MaskLID</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">LREC 2024</abbr></div> <div id="kargaran-2024-glotscript" class="col-sm-8"> <div class="title">GlotScript: A Resource and Tool for Low Resource Writing System Identification</div> <div class="author"> <b>Amir Hossein Kargaran</b>, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a style="color:red" href="" class="btn btn-sm z-depth-0" role="button">Oral Presentation</a> <a href="http://arxiv.org/abs/2309.13320" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/cisnlp/GlotScript" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/slides-glotscript-lrec-2024.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>We present GlotScript, an open resource and tool for low resource writing system identification. GlotScript-R is a resource that provides the attested writing systems for more than 7,000 languages. It is compiled by aggregating information from existing writing system resources. GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts. For an input text, it returns its script distribution where scripts are identified by ISO 15924 codes. We also present two use cases for GlotScript. First, we demonstrate that GlotScript supports cleaning multilingual corpora such as mC4 and OSCAR. Second, we analyze the tokenization of a number of language models such as GPT-4 using GlotScript and provide insights on the coverage of low resource scripts and languages by each language model. We hope that GlotScript will become a useful resource for work on low resource languages in the NLP community. GlotScript-R and GlotScript-T are available at https://github.com/cisnlp/GlotScript.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MSR 2024</abbr></div> <div id="nikeghbal-2024-girtmodel" class="col-sm-8"> <div class="title">GIRT-Model: Automated Generation of Issue Report Templates</div> <div class="author">Nafiseh Nikeghbal,  <b>Amir Hossein Kargaran</b>, and Abbas Heydarnoori </div> <div class="periodical"> <em>In 21st IEEE/ACM International Conference on Mining Software Repositories (MSR)</em> Apr 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.02632" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://doi.org/10.1145/3643991.3644906" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACM</a> <a href="https://github.com/ISE-Research/girt-model" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer’s instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model. In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP 2023</abbr></div> <div id="kargaran-2023-glotlid" class="col-sm-8"> <div class="title">GlotLID: Language Identification for Low-Resource Languages</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Ayyoob Imani, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In The 2023 Conference on Empirical Methods in Natural Language Processing</em> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.16248" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://openreview.net/forum?id=dl4e3EBz5j" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACL</a> <a href="/assets/pdf/paper-glotlid-emnlp-2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/cisnlp/GlotLID" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster-glotlid-emnlp-2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://huggingface.co/spaces/cis-lmu/glotlid-space" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Demo</a> </div> <div class="abstract hidden"> <p>Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance accessibility of NLP technology for low-resource languages and cultures. GlotLID-M model, code, and list of data sources are available: https://github.com/cisnlp/GlotLID.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL 2023</abbr></div> <div id="imanigooghari-2023-glot" class="col-sm-8"> <div class="title">Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages</div> <div class="author">Ayyoob ImaniGooghari, Peiqin Lin,  <b>Amir Hossein Kargaran</b>, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon, and Hinrich Schütze </div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a style="color:red" href="https://2023.aclweb.org/program/best_papers/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Area Chair Award</a> <a style="color:red" href="" class="btn btn-sm z-depth-0" role="button">Oral Presentation</a> <a href="http://arxiv.org/abs/2305.12182" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.acl-long.61" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACL</a> <a href="/assets/pdf/paper-glot500-acl-2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/cisnlp/Glot500" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, "help" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world’s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MSR 2023</abbr></div> <div id="nikeghbal-2023-girtdata" class="col-sm-8"> <div class="title">GIRT-Data: Sampling GitHub Issue Report Templates</div> <div class="author">Nafiseh Nikeghbal,  <b>Amir Hossein Kargaran</b>, Abbas Heydarnoori, and Hinrich Schütze </div> <div class="periodical"> <em>In IEEE/ACM 20th International Conference on Mining Software Repositories (MSR)</em> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.09236" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://ieeexplore.ieee.org/document/10174016" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">IEEE</a> <a href="https://arxiv.org/pdf/2303.09236.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/kargaranamir/girt-data" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>GitHub’s issue reports provide developers with valuable information that is essential to the evolution of a software development project. Contributors can use these reports to perform software engineering tasks like submitting bugs, requesting features, and collaborating on ideas. In the initial versions of issue reports, there was no standard way of using them. As a result, the quality of issue reports varied widely. To improve the quality of issue reports, GitHub introduced issue report templates (IRTs), which pre-fill issue descriptions when a new issue is opened. An IRT usually contains greeting contributors, describing project guidelines, and collecting relevant information. However, despite of effectiveness of this feature which was introduced in 2016, only nearly 5% of GitHub repositories (with more than 10 stars) utilize it. There are currently few articles on IRTs, and the available ones only consider a small number of repositories. In this work, we introduce GIRT-Data, the first and largest dataset of IRTs in both YAML and Markdown format. This dataset and its corresponding open-source crawler tool are intended to support research in this area and to encourage more developers to use IRTs in their repositories. The stable version of the dataset contains 1,084,300 repositories and 50,032 of them support IRTs. The stable version of the dataset and crawler is available here: https://github.com/kargaranamir/girt-data</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv 2023</abbr></div> <div id="kargaran-2023-menucraft" class="col-sm-8"> <div class="title">MenuCraft: Interactive Menu System Design with Large Language Models</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Nafiseh Nikeghbal, Abbas Heydarnoori, and Hinrich Schütze </div> <div class="periodical"> <em>arXiv preprint</em> Apr 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2303.04496" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2303.04496.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://kargaranamir.github.io/MenuCraft" class="btn btn-sm z-depth-0" role="button">Demo</a> </div> <div class="abstract hidden"> <p>Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing few-shot learning.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AACL 2022</abbr></div> <div id="kargaran-2022-hengam" class="col-sm-8"> <div class="title">Hengam: An Adversarially Trained Transformer for Persian Temporal Tagging</div> <div class="author">Sajad Mirzababaei,  <b>Amir Hossein Kargaran</b>, Hinrich Schütze, and Ehsaneddin Asgari </div> <div class="periodical"> <em>In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em> Apr 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.aacl-main.74" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACL</a> <a href="https://aclanthology.org/2022.aacl-main.74.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://www.youtube.com/watch?v=AaGBK1YRPZ4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://github.com/kargaranamir/hengam" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Many NLP main tasks benefit from an accurate understanding of temporal expressions, e.g., text summarization, question answering, and information retrieval. This paper introduces Hengam, an adversarially trained transformer for Persian temporal tagging outperforming state-of-the-art approaches on a diverse and manually created dataset. We create Hengam in the following concrete steps: (1) we develop HengamTagger, an extensible rule-based tool that can extract temporal expressions from a set of diverse language-specific patterns for any language of interest. (2) We apply HengamTagger to annotate temporal tags in a large and diverse Persian text collection (covering both formal and informal contexts) to be used as weakly labeled data. (3) We introduce an adversarially trained transformer model on HengamCorpus that can generalize over the HengamTagger’s rules. We create HengamGold, the first high-quality gold standard for Persian temporal tagging. Our trained adversarial HengamTransformer not only achieves the best performance in terms of the F1-score (a type F1-Score of 95.42 and a partial F1-Score of 91.60) but also successfully deals with language ambiguities and incorrect spellings. Our code, data, and models are publicly available at https://github.com/kargaranamir/Hengam.</p> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">WEBSCI 2021</abbr></div> <div id="kargaran-2021-wideadgraph" class="col-sm-8"> <div class="title">Wide-AdGraph: Detecting Ad Trackers with a Wide Dependency Chain Graph</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Mohammad Sadegh Akhondzadeh, Mohammad Reza Heidarpour, Mohammad Hossein Manshaei, Kave Salamatian, and Masoud Nejad Sattary </div> <div class="periodical"> <em>In 13th ACM Web Science Conference</em> Apr 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a style="color:red" href="https://www.acm.org/conferences/best-paper-awards" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Best Student Paper</a> <a href="http://arxiv.org/abs/2004.14826" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3447535.3462549" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">ACM</a> <a href="/assets/pdf/paper-wideadgraph-websci-2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF_V2</a> <a href="/assets/pdf/paper-detecting-arxiv-2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF_V1</a> <a href="https://www.youtube.com/watch?v=PpFpfEMVze0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://github.com/kargaranamir/Wide-AdGraph" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Websites use third-party ads and tracking services to deliver targeted ads and collect information about users that visit them. These services put users’ privacy at risk, and that is why users’ demand for blocking these services is growing. Most of the blocking solutions rely on crowd-sourced filter lists manually maintained by a large community of users. In this work, we seek to simplify the update of these filter lists by combining different websites through a large-scale graph connecting all resource requests made over a large set of sites. The features of this graph are extracted and used to train a machine learning algorithm with the aim of detecting ads and tracking resources. As our approach combines different information sources, it is more robust toward evasion techniques that use obfuscation or changing the usage patterns. We evaluate our work over the Alexa top-10K websites and find its accuracy to be 96.1% biased and 90.9% unbiased with high precision and recall. It can also block new ads and tracking services, which would necessitate being blocked by further crowd-sourced existing filter lists. Moreover, the approach followed in this paper sheds light on the ecosystem of third-party tracking and advertising.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ADCHEM 2021</abbr></div> <div id="kargaran-2021-alramsimilarity" class="col-sm-8"> <div class="title">Analytical Derivation and Comparison of Alarm Similarity Measures</div> <div class="author"> <b>Amir Hossein Kargaran</b>, Amir Neshastegaran, Iman Izadi, and Ehsan Yazdian </div> <div class="periodical"> <em>IFAC-PapersOnLine</em> Apr 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2003.10600" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2003.10600.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF_V2</a> <a href="/assets/pdf/paper-alarm-arxiv-2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF_V1</a> <a href="https://www.youtube.com/watch?v=8_EB3OgtE2s" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://github.com/kargaranamir/Alarm-Similarity" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.sciencedirect.com/science/article/pii/S2405896321010417" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>An industrial process includes many devices, variables, and sub-processes that are physically or electronically interconnected. These interconnections imply some level of correlation between different process variables. Since most of the alarms in a process plant are defined on process variables, alarms are also correlated. However, this can be a nuisance to operators, for one fault might trigger a, sometimes large, number of alarms. So, it is essential to find and correct correlated alarms. In this paper, we study different methods and techniques proposed to measure correlation or similarity between alarms. The similarity indices are first analytically calculated and then studied and compared. The results are also validated using Monte-Carlo simulation.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Amir Hossein Kargaran. Powered by Jekyll with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>